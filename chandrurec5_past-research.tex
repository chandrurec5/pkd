%\documentclass[oneside,8pt,onecolumn,openright]{IIScthesisPSnPDF}
\documentclass[onecolumn,12pt]{IEEEtran}
\input{pack.tex}
\usepackage{url}
%\usepackage[numbers]{natbib}
\title{\Large Past Research}
\author{Chandrashekar L}
\date{}
\begin{document}
%\Huge
%\begin{center}
%Panel of Experts
%\end{center}
\maketitle
\textbf{Thesis Title}:``Approximate Dynamic Programming and Reinforcement Learning - Algorithms, Analysis and an Application''\\
\textbf{Advisor}: Prof. Shalabh Bhatnagar (shalabh@csa.iisc.ernet.in), Department of Computer Science and Automation, Indian Institute of Science, Bangalore - 560012\\
%Background:\\
\\
\textbf{Abstract}: MDP is a useful mathematical framework to cast a variety of optimal sequential decision making problems under uncertainty in domains such as engineering, science and economics. However, computing optimal value function and optimal policy is difficult in practice because either the state space is too large or the model information is not available.\par
The primary investigations in the thesis were:
\begin{itemize}
\item \emph{Approximate Dynamic Programming} refers to a gamut of methods that compute an approximate value function and a sub-optimal policy. The thesis investigated a widely used ADP method namely the Approximate Linear Programming (ALP) formulation. In particular, analytical tools were developed to bound the performance degradation that occurs when the constraint of the ALP are reduced or approximated. The analysis is based on ideas of monotone projections in tropical linear algebra.
\item \emph{Reinforcement Learning} algorithms are stochastic approximation (SA) schemes and solve the MDP by making use of sample trajectories. Actor-Critic algorithms are two timescale SA schemes since they make use of different step-size schedules. The thesis investigated the conditions under which two timescale SA schemes are stable and convergent.
\item \emph{Crowd Sourcing} is a new mode of organizing work in multiple groups of smaller chunks of tasks and outsourcing them to a distributed and large group of people in the form of an open call. An important task attribute that affects the completion time of a task is its price, and incorrect pricing leads to task starvation. In the thesis, the pricing problem is formulated in the MDP framework to compute a pricing policy that achieves predictable completion times in simulations as well as real world experiments.
\end{itemize}
\begin{comment}
Solving MDPs typically involves computing the optimal value function and the optimal policy. However, most MDPs arising in practice have a large number of states and it is difficult to compute the optimal value function/policy. Approximate Dynamic Programming deal MDPs with large number of states by computing an approximate value function and a sub-optimal policy. The most widely used approach by ADP methods is linear function approximation wherein the value function is restricted to a subspace spanned by a set of basis functions.\par
Approximate Linear Programming (ALP) method has guaranteed performance for the sub-optimal policy it computes. A significant shortcoming of the ALP is that the number of constraints are large in the case MDPs with large number of states. In the thesis, we introduce the Generalized Reduced Linear Program
\end{comment}
\section{Publications (from the thesis)}
\subsection{Journal}
\begin{itemize}
\item Chandrashekar, L. and  Bhatnagar, S. ``A Stability Criterion for Two Timescale Stochastic Approximation Schemes", Accepted for publication in Automatica, 2017.
\item Chandrashekar, L.;  Bhatnagar, S and Szepesv\'{a}ri C., ``A Generalized Reduced Linear Program for Markov Decision Processes" (under Review in IEEE Transactions on Automatic Control).
\end{itemize}
\subsection{International Conferences}
\begin{itemize}
\item Chandrashekar, L.; Bhatnagar, S., ``Approximate Dynamic Programming with $(\min,+)$ linear function approximation for Markov Decision Processes," 53rd IEEE Annual Conference on Decision and Control (CDC), December $15-17$, $2014$, Los Angeles California, USA.
\item Chandrashekar, L.; Bhatnagar, S., ``A Generalized Reduced Linear Program for Markov Decision Processes," Twenty-Ninth AAAI Conference on Artificial Intelligence, January $25-30$, $2015$, Austin Texas, USA.
\item Chandrashekar, L.;  Dubey, A.; Bhatnagar, S. and Chithralekha, B., ``A Markov Decision Process framework for predictable job completion times on crowdsourcing platforms", Proceedings of the Second {AAAI} Conference on Human Computation and Crowdsourcing, {HCOMP} 2014, Pittsburgh, Nov. 2-4, 2014.
\item Maity, R. K.; Chandrashekar, L.;  Padakandla, S.; Bhatnagar, S., ``
Shaping Proto-Value Functions Using Rewards", European Conference on Artificial Intelligence 2016.
\end{itemize}


\bibliographystyle{plain}
\bibliography{refteach}
\end{document}

