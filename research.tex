Algorithms for  decision making under uncertainty and complexity:
The real world systems are (i) uncertain: that is, there is randomness (e.g., the time taken to service a customer in a queue, or the return-on-investment in a particular stock), and (ii) complex: too many configurations (imagine the number of possible configurations in a game of chess, or possible configurations of vehicular traffic in a big city). In the face of such uncertainty and complexity, deploying algorithms to compute the decisions could improve the performance of practical systems. The research directions includes  extending prior algorithmic design methods and  proposing newer ones to address important design questions. The design aspects and the algorithmic paradigms are described respectively in the next two paragraphs.
	The following are some of the interesting design aspects:
(i) Scalability: The number of configurations (or formally states) of the system grows exponentially in the number of dimensions. The challenge here is to design algorithms whose computational requirement scales tractably (i.e., not exponentially) with the dimensions. 
(ii) Model-Free: Designing algorithms that are purely data driven without the need for explicit model.
(iii) Finite-time performance: We need the algorithms to perform well in finite time, i.e., even the samples are limited we want algorithms that provide us meaningful answers. This direction is quite recent and most classical results are for the asymptotic regime, i.e., the case when the number of samples tend to infinity.
	The following algorithmic paradigms are of interest
(i) Reinforcement learning: algorithms combine dimensionality reduction techniques with data driven incremental updates to learn in a model free manner. 
(ii) Multi-arm-bandit: algorithms address the exploration-exploitation trade-off so as to sample efficiently without affecting performance.
(iii) Deep learning: algorithms make use of deep neural network architecture as an integral part. 

My past dealt with (a) performance guarantees for linear function approximation (a dimensionality reduction technique). (b) stability of multi-timescale reinforcement learning algorithms (c)  finite-time analysis of linear stochastic approximation algorithms, which are topics closely related to the aforementioned design aspects as well as algorithmic paradigms. My current research deals with coming up with a useful theory for the generalization performance of deep neural networks.
