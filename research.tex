Algorithms for  decision making under uncertainty and complexity has been my core area of interest. Here, I present a big picture view of the research directions that I am likely to pursue. The real world systems are (i) uncertain: that is, there is randomness (e.g., the time taken to service a customer in a queue, or the return-on-investment in a particular stock), and (ii) complex: too many configurations (imagine the number of possible configurations in a game of chess, or possible configurations of vehicular traffic in a big city). In the face of such uncertainty and complexity, deploying algorithms to compute the decisions could improve the performance of practical systems. A key question is: can we design 'robust' algorithms? Here, the term 'robust' is used in a broader sense and means one of the following research directions:
(i) Scalability: The number of configurations (or formally states) of the system grows exponentially in the number of dimensions. The challenge here is to design algorithms whose computational requirement scales tractably (i.e., not exponentially) with the dimensions. Typ

 However, in complex real world systems, due to the curse-of-dimensionality deriving such decision rules often become computationally intractable. While dimensionality free methods [3] have had partial success in alleviating this problem, it is still an active area of research. More importantly, with the advent of the big data era (http://dst.gov.in/big-data-initiative-1), it is crucial to develop decision making algorithms which are data driven as opposed to just being model based.  
	
	
	

	The paradigm of reinforcement learning (RL) algorithms combines ideas of dimensionality reduction and learn in a model-free fashion based on sample based incremental learning schemes. There are several interesting questions: 
	
 	Deep learning has been successful recently, and a particular aspect 
	

Dimensionality Reduction: Curse-of-dimensionality denotes the fact that the number of states in the system grows exponentially in the number of state variables. Function approximation is a widely used dimensionality reduction wherein the each state is represented by a feature vector and computations are carried by taking linear combinations of the features. Dimensionality reduction is achieved by letting the dimension of the feature vector to be much less than the number of states.
Approximate Dynamic Programming: (ADP) algorithms [3] tackle the complexity of systems (i.e, of large number of states) by cleverly combining approximate representations and mathematical optimization. Reinforcement Learning: In most cases, the underlying system model in not known explicitly. How- ever, the samples can be obtained via simulation or direct interaction with the system. Reinforcement Learning (RL) algorithms [16] learn the optimal decision via feedback obtained by directly interacting with the system. RL algorithms such as Q-learning [18], temporal difference learning [17] have been
successful in domains such as Backgammon, elevator control etc.
III. OBJECTIVE: OPEN PROBLEMS AND POTENTIAL DIRECTIONS We now list below several research gaps that we propose to address (Table I).
1) ADP with Performance Guarantees: Since the ADP methods are not exact, in most cases they only compute approximate or sub-optimal decision rules. Several well known ADP algorithms suffer from convergence issues [4] and the performance loss of the sub-optimal policy cannot be ascertained. However, some of our recent works [11, 14] have shown conditions under which ADP algorithms are convergent and yield provably good decision rule. The focus here is to explore newer ADP algorithms and approximation architectures that can be guaranteed to converge and yield a good decision rule.
2) ADP for Constrained and Risk Sensitive MDPs: The discounted cost and the average cost are the most widely studied formulations in the classical MDP setting. However, in practice the decisions need to balance multiple costs and sensitivities to the various costs can be different. Constrained MDP formulation allows us to take into account the various costs involved. However, ADP algorithms for constrained MDPs have not been sufficiently explored. The Approximate Linear Programming (ALP) [12] approach to MDP can accommodate multiple cost functions. A possible research direction is to extend [14] to constrained MDP to derive new ADP algorithms. Risk sensitive formulation of the