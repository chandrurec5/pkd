\begin{thebibliography}{10}

\bibitem{convex}
A.~Agarwal, D.~P. Foster, D.~J. Hsu, S.~M. Kakade, and A.~Rakhlin.
\newblock Stochastic convex optimization with bandit feedback.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1035--1043, 2011.

\bibitem{auer}
P.~Auer, N.~Cesa-Bianchi, Y.~Freund, and R.~E. Schapire.
\newblock Gambling in a rigged casino: The adversarial multi-armed bandit
  problem.
\newblock In {\em Foundations of Computer Science, 1995. Proceedings., 36th
  Annual Symposium on}, pages 322--331. IEEE, 1995.

\bibitem{dpchapter}
D.~P. Bertsekas.
\newblock Approximate dynamic programming.
\newblock In {\em Dynamic Programming and Optimal Control}, volume~II. Athena
  Scientific, Belmont, MA, 3rd edition, 2011.

\bibitem{BertB}
D.~P. Bertsekas.
\newblock {\em Dynamic Programming and Optimal Control}, volume~II.
\newblock Athena Scientific, Belmont, MA, 4th edition, 2013.

\bibitem{spsa}
S.~Bhatnagar, M.~C. Fu, S.~I. Marcus, and I.~Wang.
\newblock Two-timescale simultaneous perturbation stochastic approximation
  using deterministic perturbation sequences.
\newblock {\em ACM Trans. Model. Comput. Simul.}, 13:180--209, April 2003.

\bibitem{const}
V.~S. Borkar.
\newblock An actor-critic algorithm for constrained markov decision processes.
\newblock {\em Systems \& control letters}, 54(3):207--213, 2005.

\bibitem{sa}
V.~S. Borkar.
\newblock {\em Stochastic Approximation: A Dynamical Systems Viewpoint}.
\newblock TRIM, 2008.

\bibitem{borkarQ}
V.~S. Borkar and S.~P. Meyn.
\newblock Risk-sensitive optimal control for markov decision processes with
  monotone cost.
\newblock {\em Math. Oper. Res.}, 27(1):192--209, 2002.

\bibitem{sw}
Q.~Cao, T.~Abdelzaher, T.~He, and J.~Stankovic.
\newblock Towards optimal sleep scheduling in sensor networks for rare-event
  detection.
\newblock In {\em Proceedings of the 4th international symposium on Information
  processing in sensor networks}, page~4. IEEE Press, 2005.

\bibitem{hcompwork}
L.~Chandrashekar, A.Dubey, S.~Bhatnagar, and B.~Chithralekha.
\newblock A markov decision process framework for predictable job completion
  times on crowdsourcing platforms.
\newblock In {\em Proceedings of the Seconf {AAAI} Conference on Human
  Computation and Crowdsourcing, {HCOMP} 2014, November 2-4, 2014, Pittsburgh,
  Pennsylvania, {USA}}, 2014.

\bibitem{cdcwork}
L.~Chandrashekar and S.~Bhatnagar.
\newblock Approximate dynamic programming with (min; +) linear function
  approximation for markov decision processes.
\newblock In {\em 53rd {IEEE} Conference on Decision and Control, {CDC} 2014,
  Los Angeles, CA, USA, December 15-17, 2014}, pages 1588--1593, 2014.

\bibitem{alp}
D.~P. de~Farias and B.~Van Roy.
\newblock The linear programming approach to approximate dynamic programming.
\newblock {\em Operations Research}, 51(6):850--865, 2003.

\bibitem{smpl}
S.~S. Farahani, T.~van~den Boom, and B.~De Schutter.
\newblock Exact and approximate approaches to the identification of stochastic
  max-plus-linear systems.
\newblock {\em Discrete Event Dynamic Systems}, 24(4):447--471, 2014.

\bibitem{aaaiwork}
C.~Lakshminarayanan and S.~Bhatnagar.
\newblock A generalized reduced linear program for markov decision processes.
\newblock In {\em Proceedings of the Twenty-Ninth {AAAI} Conference on
  Artificial Intelligence, January 25-30, 2015, Austin, Texas, {USA.}}, pages
  2722--2728, 2015.

\bibitem{mH1}
P.~Liao, P.~Klasnja, A.~Tewari, and S.~A. Murphy.
\newblock Sample size calculations for micro-randomized trials in mhealth.
\newblock {\em Statistics in medicine}, 2015.

\bibitem{sutton}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Introduction to Reinforcement Learning}.
\newblock MIT Press, Cambridge, MA, USA, 1st edition, 1998.

\bibitem{Tsit}
J.~N. Tsitsiklis and B.~Van Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock 1997.

\bibitem{qlearn}
C.~J. Watkins and P.~Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\end{thebibliography}
